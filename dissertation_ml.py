# -*- coding: utf-8 -*-
"""Dissertation - ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gQdbs0yvFSMwOYQNXa7o-EnbtvmLJ2XA

# Initial Setup
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import random
import tqdm
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.models import *
from keras.layers import *
from keras.callbacks import *
from keras import backend as K
from keras.optimizers import Adam
from keras.layers import Dense, Embedding, SimpleRNN
from keras.models import Sequential
from scipy import stats
from pprint import pprint
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import *
from sklearn.utils import resample
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline

from pylab import plt, mpl
plt.style.use('seaborn')
mpl.rcParams['savefig.dpi'] = 300
mpl.rcParams['font.family'] = 'serif'
os.environ['PYTHONHASHSEED'] = '0'

import os
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# %matplotlib inline

np.set_printoptions(precision=4, suppress=True)

tf.random.set_seed(100)

import tensorflow as tf
tf.test.gpu_device_name()

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
  except RuntimeError as e:
    print(e)

from google.colab import drive

# Mounting the Google drive to read the data
drive.mount('/drive')

# Data Importing
df = pd.read_excel('/drive/My Drive/Colab Notebooks/Data/Climortgage-7.xlsx')

unique_counts = df['new_DLQ_STATUS'].value_counts()
print(unique_counts)

print(df.columns)

df.head(10)

"""# Logistic Regression - 1A"""

y = df['new_DLQ_STATUS']  # Correcting the target variable

# Defining the features
features = ['ORIG_RATE', 'ORIG_UPB', 'FIRST_FLAG', 'OLTV', 'DTI',
            'CSCORE_B', 'CSCORE_C','Weather_Events']

X = df[features]

X.head()

import statsmodels.api as sm
X1=sm.add_constant(X)
logit_model=sm.Logit(y,X1)
result=logit_model.fit()
print(result.summary2())

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25,stratify=y)
X_train.head()

y_train.head()

lr = LogisticRegression()
lr.fit(X_train, y_train)
preds = lr.predict_proba(X_test)

preds.shape

preds[0]

y_probas = preds[:, 1]

from sklearn.metrics import roc_auc_score
roc = roc_auc_score(y_test,y_probas)
print('ROC= ', roc)

# figure
import sklearn.metrics as metrics
fpr, tpr, threshold = metrics.roc_curve(y_test,y_probas)
log_auc = pd.DataFrame()
log_auc["log_tpr"] = tpr
log_auc["log_fpr"] = fpr
log_auc["log_threshold"] = threshold

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.rcParams['font.sans-serif'] = ['Arial']
plt.rcParams['axes.unicode_minus'] = False
# plt.rcParams['savefig.dpi'] = 300
# plt.rcParams['figure.dpi'] = 300
# plt.figure(figsize=(10,10))
plt.plot(log_auc['log_fpr'], log_auc['log_tpr'], 'gold', label = "LR -"+ f" AUC ={roc.round(4)}")
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'p--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.tick_params()
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

# Assuming your data is in a pandas DataFrame called df
descriptive_stats = df.describe(include='all')

# Print the descriptive statistics
print(descriptive_stats)

from statsmodels.stats.outliers_influence import variance_inflation_factor

# the independent variables set
X = df[['ORIG_RATE', 'ORIG_UPB','OLTV', 'DTI',
            'CSCORE_B', 'CSCORE_C', 'FIRST_FLAG', 'Weather_Events']]

# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

print(vif_data)

import pandas as pd

# assuming 'df' is your DataFrame
df = df[['ORIG_RATE', 'ORIG_UPB','OLTV', 'DTI', 'CSCORE_B', 'CSCORE_C', 'FIRST_FLAG', 'Weather_Events','new_DLQ_STATUS', 'Hurricanes<3', 'Hurricanes>3', 'Cyclones']]

# calculate the correlation matrix
correlation_matrix = df.corr()

# to display the matrix
print(correlation_matrix)

import statsmodels.api as sm
X_test1=sm.add_constant(X_test)
logit_model22=sm.Logit(y_test,X_test1)
result1=logit_model22.fit()
print(result1.summary2())

"""# Logistic Regression - 2A"""

y = df['new_DLQ_STATUS']  # Correcting the target variable

# Defining the features
feature1 = ['ORIG_RATE', 'ORIG_UPB','OLTV', 'DTI',
            'CSCORE_B', 'CSCORE_C', 'FIRST_FLAG']

U = df[feature1]

U.head()

import statsmodels.api as sm
U1=sm.add_constant(U)
logit_model1=sm.Logit(y,U1)
result=logit_model1.fit()
print(result.summary2())

U_train, U_test, y_train, y_test = train_test_split(U, y, random_state=42, test_size=0.25,stratify=y)
U_train.head()

y_train.head()

lr = LogisticRegression()
lr.fit(U_train, y_train)
preds1 = lr.predict_proba(U_test)

preds1.shape

preds1[0]

y_probas = preds1[:, 1]

from sklearn.metrics import roc_auc_score
roc = roc_auc_score(y_test,y_probas)
print('ROC= ', roc)

import matplotlib.pyplot as plt
import pandas as pd

# Convert 'FIRST_PAYMENT' column to datetime format
df['FIRST_PAYMENT] = pd.to_datetime(df['FIRST_PAYM

# Set 'FIRST_PAYMENT' as the index
df.set_index('FIRST_PAYMENT', inplace=True)

# Group the data by month and calculate the sum of actual and predicted defaults
grouped_base = df.resample('M')[['new_DLQ_STATUS', 'predicted_default_base']].sum()
grouped_weather = df.resample('M')[['new_DLQ_STATUS', 'predicted_default_weather']].sum()

# Create subplots
fig, ax = plt.subplots(2, 1, figsize=(12, 12))

# Change the background color to white
fig.patch.set_facecolor('white')
for axes in ax:
    axes.set_facecolor('white')

# Plot actual vs predicted defaults over time for base model
ax[0].plot(grouped_base.index, grouped_base['new_DLQ_STATUS'], label='Actual', color='black')
ax[0].plot(grouped_base.index, grouped_base['predicted_default_base'], label='Predicted', linestyle='--', color='blue')
ax[0].set_xlabel('Time', weight='bold')
ax[0].set_ylabel('Number of Defaults', weight='bold')
ax[0].set_title('Actual vs Predicted Loan Defaults Over Time - Base Model')
ax[0].legend()

# Plot actual vs predicted defaults over time for model including weather
ax[1].plot(grouped_weather.index, grouped_weather['new_DLQ_STATUS'], label='Actual', color='black')
ax[1].plot(grouped_weather.index, grouped_weather['predicted_default_weather'], label='Predicted', linestyle='--', color='blue')
ax[1].set_xlabel('Time', weight='bold')
ax[1].set_ylabel('Number of Defaults', weight='bold')
ax[1].set_title('Actual vs Predicted Loan Defaults Over Time - Model with Weather Events')
ax[1].legend()

plt.tight_layout()

# Save the figure before showing it
plt.savefig('plot.png')

plt.show()

# Download the figure
from google.colab import files
files.download('plot.png')

# Import libraries
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Generate predictions
df['predicted_default_base'] = result_base.predict(U1)
df['predicted_default_weather'] = result_weather.predict(X1)

# Group the data by month and calculate the sum of actual and predicted defaults
grouped_base = df.resample('M', on='FIRST_PAYMENT')[['new_DLQ_STATUS', 'predicted_default_base']].sum()
grouped_weather = df.resample('M', on='FIRST_PAYMENT')[['new_DLQ_STATUS', 'predicted_default_weather']].sum()

# Create subplots
fig, ax = plt.subplots(2, 1, figsize=(12, 12))

# Change the background color to white
fig.patch.set_facecolor('white')
for axes in ax:
    axes.set_facecolor('white')

# Plot actual vs predicted defaults over time for base model
ax[0].plot(grouped_base.index, grouped_base['new_DLQ_STATUS'], label='Actual', color='black')
ax[0].plot(grouped_base.index, grouped_base['predicted_default_base'], label='Predicted', linestyle='--', color='blue')
ax[0].set_xlabel('Time', weight='bold')
ax[0].set_ylabel('Number of Defaults', weight='bold')
ax[0].set_title('Actual vs Predicted Loan Defaults Over Time - Base Model')
ax[0].legend()

# Plot actual vs predicted defaults over time for model including weather
ax[1].plot(grouped_weather.index, grouped_weather['new_DLQ_STATUS'], label='Actual', color='black')
ax[1].plot(grouped_weather.index, grouped_weather['predicted_default_weather'], label='Predicted', linestyle='--', color='blue')
ax[1].set_xlabel('Time', weight='bold')
ax[1].set_ylabel('Number of Defaults', weight='bold')
ax[1].set_title('Actual vs Predicted Loan Defaults Over Time - Model with Weather Events')
ax[1].legend()

plt.tight_layout()

# Save the figure before showing it
plt.savefig('plot.png')

plt.show()

# Download the figure
from google.colab import files
files.download('plot.png')

df['FIRST_PAYMENT'] = pd.to_datetime(df['FIRST_PAYMENT'])

# Fit the model
result_base = logit_model1.fit()
result_weather = logit_model.fit()

# Predict
df['predicted_default_base'] = result_base.predict(U1)
df['predicted_default_weather'] = result_weather.predict(X1)

import statsmodels.api as sm
U2=sm.add_constant(U_test)
logit_model33=sm.Logit(y_test,U2)
result33=logit_model33.fit()
print(result33.summary2())

"""# RF - 1B"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score



# Explanatory variables
T = df[['ORIG_RATE','ORIG_UPB','FIRST_FLAG','OLTV','DTI','CSCORE_B','CSCORE_C','Weather_Events']]
TextVectorization = StandardScaler().fit_transform(T)  # It's a good practice to standardize your variables

# Target variable
y = df['new_DLQ_STATUS']

from sklearn.model_selection import train_test_split

T_train, T_test, y_train, y_test = train_test_split(T, y, test_size=0.33, random_state=42)

# Use SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
T_train_res, y_train_res = smote.fit_resample(T_train, y_train)

# Random Forest model
rf_model2 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model2.fit(T_train_res, y_train_res)

from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold


# Use SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
T_train_res, y_train_res = smote.fit_resample(T_train, y_train)

# Define the parameter grid for hyperparameter tuning
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Define StratifiedKFold object
stratified_kfold = StratifiedKFold(n_splits=3)

# RandomizedSearchCV to find the best hyperparameters, using parallelization
random_search = RandomizedSearchCV(estimator=rf_model2, param_distributions=param_dist, cv=stratified_kfold,
                                   scoring='roc_auc_ovr', n_jobs=-1, n_iter=10, random_state=42)
random_search.fit(T_train_res, y_train_res)

# Use the best model from random search to make predictions
best_rf_model = random_search.best_estimator_
y_pred = best_rf_model.predict(T_test)

# Compute probabilities for all classes
y_pred_proba = best_rf_model.predict_proba(T_test)

# Choose the probability for the class of interest
class_index = 1  # change this to the index of the class of interest
y_pred_proba_class = y_pred_proba[:, class_index]

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Compute and print AUC score
auc_score = roc_auc_score(y_test, y_pred_proba_class, multi_class='ovr')
print("AUC: ", auc_score)

from sklearn.metrics import roc_curve

y_scores = rf_model2.predict_proba(T_test)[:, 1]
# Assuming y_test are your true labels and y_scores are the probability estimates of the positive class
fpr, tpr, thresholds = roc_curve(y_test, y_scores)

# Now you can calculate KS statistic
KS_statistic = max(tpr - fpr)
print("KS Statistic:", KS_statistic)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing
import StandardScaler

# Define the features
features2 = ['ORIG_RATE','ORIG_UPB','FIRST_FLAG','OLTV','DTI','CSCORE_B','CSCORE_C','Weather_Events']

# Create a numpy array V by standardizing your dataframe
T = df[features2]
T = StandardScaler().fit_transform(T)  # It's a good practice to standardize your variables

# Create a DataFrame from the standardized numpy array
T_df = pd.DataFrame(T, columns=features2)

# Fit the model
rf_model2.fit(T_df, y)

# Calculate importances and std deviation
importances = rf_model2.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf_model2.estimators_], axis=0)

# Get the indices of the features sorted by importance
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(len(features2)):
    print("%d. feature %s (%f)" % (f + 1, features2[indices[f]], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature Importance (Base Model + Weather)")
plt.bar(range(len(features2)), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(len(features2)), np.array(features2)[indices], rotation=90)
plt.xlim([-1, len(features2)])
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Base Model
rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(T, y)
df['predicted_default_base_rf'] = rf_model1.predict_proba(T)[:,1] # We are interested in the probability of the class being '1'

# Model including weather
rf_model2 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model2.fit(T, y)
df['predicted_default_weather_rf'] = rf_model2.predict_proba(T)[:,1]

grouped_base_rf = df.resample('M', on='FIRST_PAYMENT')[['new_DLQ_STATUS', 'predicted_default_base_rf']].sum()
grouped_weather_rf = df.resample('M', on='FIRST_PAYMENT')[['new_DLQ_STATUS', 'predicted_default_weather_rf']].sum()

# Create subplots
fig, ax = plt.subplots(2, 1, figsize=(12, 12))

# Change the background color to white
fig.patch.set_facecolor('white')
for axes in ax:
    axes.set_facecolor('white')

# Plot actual vs predicted defaults over time for base model
ax[0].plot(grouped_base_rf.index, grouped_base_rf['new_DLQ_STATUS'], label='Actual', color='black')
ax[0].plot(grouped_base_rf.index, grouped_base_rf['predicted_default_base_rf'], label='Predicted', linestyle='--', color='blue')
ax[0].set_xlabel('Time', weight='bold')
ax[0].set_ylabel('Number of Defaults', weight='bold')
ax[0].set_title('Actual vs Predicted Loan Defaults Over Time - Random Forest Base Model')
ax[0].legend()

# Plot actual vs predicted defaults over time for model including weather
ax[1].plot(grouped_weather_rf.index, grouped_weather_rf['new_DLQ_STATUS'], label='Actual', color='black')
ax[1].plot(grouped_weather_rf.index, grouped_weather_rf['predicted_default_weather_rf'], label='Predicted', linestyle='--', color='blue')
ax[1].set_xlabel('Time', weight='bold')
ax[1].set_ylabel('Number of Defaults', weight='bold')
ax[1].set_title('Actual vs Predicted Loan Defaults Over Time - Random Forest Model with Weather Events')
ax[1].legend()

plt.tight_layout()

# Save the figure before showing it
plt.savefig('plot1.png')

plt.show()

# Download the figure
from google.colab import files
files.download('plot1.png')

from sklearn.metrics import accuracy_score, log_loss

# Predict on train and test sets
y_train_pred = rf_model1.predict(T_train)
y_test_pred = rf_model1.predict(T_test)

# Calculate metrics
train_accuracy1 = accuracy_score(y_train, y_train_pred)
test_accuracy1 = accuracy_score(y_test, y_test_pred)

# Print out the results
print("Training accuracy: ", train_accuracy1)
print("Test accuracy: ", test_accuracy1)

# If the difference between the training accuracy and test accuracy is large, that's a sign of overfitting

"""# RF - 2B"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Explanatory variables
V = df[['ORIG_RATE','ORIG_UPB','FIRST_FLAG','OLTV','DTI','CSCORE_B','CSCORE_C']]
V = StandardScaler().fit_transform(V)  # It's a good practice to standardize your variables

# Target variable
y = df['new_DLQ_STATUS']

V_train, V_test, y_train1, y_test1 = train_test_split(V, y, test_size=0.33, random_state=42)

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(V_train, y_train1)

from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold


# Use SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
V_train_res, y_train1_res = smote.fit_resample(V_train, y_train1)

# Define the parameter grid for hyperparameter tuning
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Define StratifiedKFold object
stratified_kfold = StratifiedKFold(n_splits=3)

# RandomizedSearchCV to find the best hyperparameters, using parallelization
random_search1 = RandomizedSearchCV(estimator=rf_model1, param_distributions=param_dist, cv=stratified_kfold,
                                   scoring='roc_auc_ovr', n_jobs=-1, n_iter=10, random_state=42)
random_search1.fit(V_train_res, y_train1_res)


# Use the best model from random search to make predictions
best_rf_model1 = random_search1.best_estimator_
y_pred1 = best_rf_model1.predict(V_test)

# Compute probabilities for all classes
y_pred1_proba = best_rf_model1.predict_proba(V_test)

# Choose the probability for the class of interest
class_index = 1  # change this to the index of the class of interest
y_pred1_proba_class = y_pred1_proba[:, class_index]

print("Accuracy:", accuracy_score(y_test1, y_pred1))
print("Confusion Matrix:\n", confusion_matrix(y_test1, y_pred1))
print("Classification Report:\n", classification_report(y_test1, y_pred1))

# Compute and print AUC score
auc_score = roc_auc_score(y_test1, y_pred1_proba_class, multi_class='ovr')
print("AUC: ", auc_score)

from sklearn.metrics import roc_curve
# Getting prediction probabilities for the positive class
y_scores1 = rf_model1.predict_proba(V_test)[:, 1]

# Getting ROC Curve values
fpr, tpr, thresholds = roc_curve(y_test1, y_scores1)

# Getting the KS Statistic
KS_statistic = max(tpr - fpr)

print("KS Statistic:", KS_statistic)

from sklearn.ensemble import RandomForestClassifier

# Assume X_train are your features and y_train are your labels
rf_model1 = RandomForestClassifier(n_estimators=100)
rf_model1.fit(V_train, y_train1)

# Get importance
importance = rf_model1.feature_importances_

# summarize feature importance
for i, j in enumerate(importance):
    print('Feature: %0d, Score: %.5f' % (i, j))

from sklearn.metrics import accuracy_score, log_loss

# Predict on train and test sets
y_train_pred1 = rf_model1.predict(V_train)
y_test_pred1 = rf_model1.predict(V_test)

# Calculate metrics
train_accuracy = accuracy_score(y_train1, y_train_pred1)
test_accuracy = accuracy_score(y_test1, y_test_pred1)

# Print out the results
print("Training accuracy: ", train_accuracy)
print("Test accuracy: ", test_accuracy)

# If the difference between the training accuracy and test accuracy is large, that's a sign of overfitting

import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler

# Define the features
features = ['ORIG_RATE','ORIG_UPB','FIRST_FLAG','OLTV','DTI','CSCORE_B','CSCORE_C']

# Create a numpy array V by standardizing your dataframe
V = df[features]
V = StandardScaler().fit_transform(V)  # It's a good practice to standardize your variables

# Create a DataFrame from the standardized numpy array
V_df = pd.DataFrame(V, columns=features)

# Fit the model
rf_model1.fit(V_df, y)

# Calculate importances and std deviation
importances = rf_model1.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf_model1.estimators_], axis=0)

# Get the indices of the features sorted by importance
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(len(features)):
    print("%d. feature %s (%f)" % (f + 1, features[indices[f]], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature Importance (Base Model)")
plt.bar(range(len(features)), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(len(features)), np.array(features)[indices], rotation=90)
plt.xlim([-1, len(features)])
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler

# Hyperparameters to tune
param_dist = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}

for i, dataframe in enumerate(df_list):
    event = events[i]
    # Prepare input data
    P = dataframe[selected_variables]
    Y = dataframe['new_DLQ_STATUS']

    P_train, P_test, Y_train, Y_test = train_test_split(P, Y, test_size=0.2, random_state=42)

    # Apply SMOTE for class imbalance
    sm = SMOTE(random_state=42)
    P_train_res, Y_train_res = sm.fit_resample(P_train, Y_train)

    P_train_res = StandardScaler().fit_transform(P_train_res)
    P_test = StandardScaler().fit_transform(P_test)

    # Initialize Logistic Regression
    logreg = LogisticRegression(max_iter=1000)

    # Perform Random Search
    random_search = RandomizedSearchCV(logreg, param_dist, cv=5, scoring='roc_auc', n_iter=10, random_state=42)
    random_search.fit(P_train_res, Y_train_res)

    # Use the best model from random search to make predictions
    best_log_model = random_search.best_estimator_
    Y_pred = best_log_model.predict(P_test)

    # Compute AUC
    roc_auc = roc_auc_score(Y_test, Y_pred)

    print(f"Logistic Regression tuned parameters for {event}: {random_search.best_params_}")
    print(f"Logistic Regression AUC for {event}: {roc_auc}")

"""# Logistic Regression - 3A"""

y = df['new_DLQ_STATUS']  # Correcting the target variable

# Defining the features
feature21 = ['ORIG_RATE', 'ORIG_UPB', 'OLTV','DTI','CSCORE_B', 'CSCORE_C','FIRST_FLAG', 'Hurricanes>3' ,'Cyclones', 'Hurricanes<3']

LRT = df[feature21]

import statsmodels.api as sm
LRT1=sm.add_constant(LRT)
logit_model=sm.Logit(y,LRT1)
result=logit_model.fit()
print(result.summary2())

LRT_train, LRT_test, y_train, y_test = train_test_split(LRT, y, random_state=42, test_size=0.25,stratify=y)
LRT_train.head()

lr = LogisticRegression()
lr.fit(LRT_train, y_train)
preds2 = lr.predict_proba(LRT_test)

preds2[0]

y_probas = preds2[:, 1]

from sklearn.metrics import roc_auc_score
roc = roc_auc_score(y_test,y_probas)
print('ROC= ', roc)

import statsmodels.api as sm
LRT2=sm.add_constant(LRT_test)
logit_model=sm.Logit(y_test,LRT2)
result1=logit_model.fit()
print(result1.summary())

"""# RF - 3B"""

# Explanatory variables
RF3 = df[['ORIG_RATE', 'ORIG_UPB', 'OLTV','DTI','CSCORE_B', 'CSCORE_C','FIRST_FLAG', 'Hurricanes>3' ,'Cyclones', 'Hurricanes<3']]
RF3 = StandardScaler().fit_transform(RF3)  # It's a good practice to standardize your variables

# Target variable
y = df['new_DLQ_STATUS']

RF3_train, RF3_test, y_train2, y_test2 = train_test_split(RF3, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

rf_model4 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model4.fit(RF3_train, y_train2)

from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV

# ... (rest of the code remains the same)

# Use SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
RF3_train_res, y_train2_res = smote.fit_resample(RF3_train, y_train2)

# Define the parameter grid for hyperparameter tuning
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# RandomizedSearchCV to find the best hyperparameters, using parallelization
random_search = RandomizedSearchCV(estimator=rf_model4, param_distributions=param_dist, cv=5,
                                   scoring='roc_auc_ovr', n_jobs=-1, n_iter=10, random_state=42)
random_search.fit(RF3_train_res, y_train2_res)

# Use the best model from random search to make predictions
best_rf_model = random_search.best_estimator_
y_pred = best_rf_model.predict(RF3_test)

# Compute probabilities for all classes
y_pred_proba = best_rf_model.predict_proba(RF3_test)

# Choose the probability for the class of interest
class_index = 1  # change this to the index of the class of interest
y_pred_proba_class = y_pred_proba[:, class_index]

print("Accuracy:", accuracy_score(y_test2, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test2, y_pred))
print("Classification Report:\n", classification_report(y_test2, y_pred))

# Compute and print AUC score
auc_score = roc_auc_score(y_test2, y_pred_proba_class, multi_class='ovr')
print("AUC: ", auc_score)

from sklearn.metrics import roc_curve

# Assuming you have a trained random forest model "rf_model" and test data "V_test" and "y_test"

# Getting prediction probabilities for the positive class
y_scores2 = rf_model4.predict_proba(RF3_test)[:, 1]

# Getting ROC Curve values
fpr, tpr, thresholds = roc_curve(y_test2, y_scores2)

# Getting the KS Statistic
KS_statistic = max(tpr - fpr)

print("KS Statistic:", KS_statistic)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler

# Define the features
features3 = ['ORIG_RATE', 'ORIG_UPB', 'OLTV','DTI','CSCORE_B', 'FIRST_FLAG', 'CSCORE_C', 'Hurricanes>3' ,'Cyclones', 'Hurricanes<3']

# Create a numpy array V by standardizing your dataframe
RF3 = df[features3]
RF3 = StandardScaler().fit_transform(RF3)  # It's a good practice to standardize your variables

# Create a DataFrame from the standardized numpy array
RF3_df = pd.DataFrame(RF3, columns=features3)

# Fit the model
rf_model4.fit(RF3_df, y)

# Calculate importances and std deviation
importances = rf_model4.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf_model4.estimators_], axis=0)

# Get the indices of the features sorted by importance
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(len(features3)):
    print("%d. feature %s (%f)" % (f + 1, features3[indices[f]], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature Importance (Seperate Weather Events)")
plt.bar(range(len(features3)), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(len(features3)), np.array(features3)[indices], rotation=90)
plt.xlim([-1, len(features3)])
plt.show()

"""# RF Comparision"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Mapping of event names to their ZIP codes and affected states
event_zip_codes = {
    'Dorian': {'zip_codes': [320, 321, 329, 336, 334, 314, 285, 328, 284, 315, 279, 278, 275, 299, 294, 295], 'states': ['FL', 'NC', 'SC']},
    'Michael': {'zip_codes': [326, 327, 324], 'states': ['FL']},
    'Florence': {'zip_codes': [295, 285, 284, 275, 278, 279, 294, 249, 280, 290, 291], 'states': ['NC', 'SC']},
    '2018_Cyclone': {'zip_codes': [82, 74, 74, 74, 88, 88, 77, 77, 77, 77, 77, 77, 82, 82, 84, 82, 73, 73, 73, 85, 82, 60, 70, 117], 'states': ['NY', 'NJ']},
    'Nate': {'zip_codes': [324, 325, 325], 'states': ['FL']},
    'Irma': {'zip_codes': [322, 321, 339, 324, 320, 329, 325, 294, 295], 'states': ['FL', 'SC']},
    '2016_Cyclone': {'zip_codes': [77, 82, 88, 87, 89, 64, 80, 83, 84, 78, 100, 112, 117, 100, 113, 114, 115, 117, 119, 105], 'states': ['NY', 'NJ']},
    'Matthew': {'zip_codes': [321, 334, 322, 342, 299, 292, 295, 303, 320, 294, 285, 279, 329, 284, 291, 283], 'states': ['FL', 'SC', 'NC']},
}

# Create new columns for the hurricane categories
df['Hurricane>3'] = df['ZIP'].apply(lambda x: int(x in event_zip_codes['Dorian']['zip_codes'] or x in event_zip_codes['Michael']['zip_codes']))
df['Hurricane<3'] = df['ZIP'].apply(lambda x: int(x in event_zip_codes['Florence']['zip_codes'] or x in event_zip_codes['Nate']['zip_codes'] or x in event_zip_codes['Irma']['zip_codes'] or x in event_zip_codes['Matthew']['zip_codes']))
df['Cyclones'] = df['ZIP'].apply(lambda x: int(x in event_zip_codes['2018_Cyclone']['zip_codes'] or x in event_zip_codes['2016_Cyclone']['zip_codes']))

# Mapping of event categories to their respective columns and states affected
event_cols = {
    'Hurricane>3': {'column': 'Hurricane>3', 'events': ['Dorian', 'Michael']},
    'Hurricane<3': {'column': 'Hurricane<3', 'events': ['Florence', 'Nate', 'Irma', 'Matthew']},
    'Cyclone': {'column': 'Cyclones', 'events': ['2018_Cyclone', '2016_Cyclone']}
}

# Columns to be used for Random Forest analysis
base_features = ['ORIG_RATE', 'ORIG_UPB', 'OLTV', 'DTI', 'CSCORE_B', 'CSCORE_C', 'FIRST_FLAG']

# Loop through each weather event category
for category, event_info in event_cols.items():
    event_column = event_info['column']
    event_states = [state for event in event_info['events'] for state in event_zip_codes[event]['states']]

    # Add the event column to the feature list
    features = base_features + [event_column]

    # Filter DataFrame based on the event category and states affected
    df_subset = df[(df[event_column] == 1) & (df['STATE'].isin(event_states))]

    # Define features (X) and target (y)
    X = df_subset[features]
    y = df_subset['new_DLQ_STATUS']

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define and fit the model
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    # Print classification report
    y_pred = model.predict(X_test)
    print(f"Classification Report for {category}:\n")
    print(classification_report(y_test, y_pred))

    # Print feature importance
    print(f"\nFeature Importance for {category}:\n")
    for feature, importance in zip(features, model.feature_importances_):
        print(f"{feature}: {importance:.2f}")

"""# SHAP - 3C"""

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

# Split features and target
xx = df.drop(['LOAN_ID', 'SELLER', 'DLQ_STATUS', 'FIRST_PAYMENT','STATE', 'ZIP', 'new_DLQ_STATUS', 'Weather_Events', 'Hurricanes<3', 'Hurricanes>3'  ], axis=1)
yy = df[['new_DLQ_STATUS']]

# Split into train and test
xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.2, random_state=123)

# Construct a NN classification model by TensorFlow
with tf.device('/GPU:0'):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_dim=xx_train.shape[1]))  # Adjust input_dim
    model.add(Dense(1, activation='sigmoid'))  # Use sigmoid activation for binary classification
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Use binary_crossentropy for binary classification
    early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)
    model.fit(xx_train, yy_train, epochs=50, validation_data=(xx_test, yy_test), verbose=True, callbacks=[early_stop])

!pip install shap
import shap

# Define the explainer
# Shap has many explainers, Kernelexplainer supports tensorflow NN model
explainer = shap.KernelExplainer(model,xx_train.iloc[:50,:])

# Calculate the SHAP values for each feature using 500 rows,
shap_values = explainer.shap_values(xx_train.iloc[:500,:])

# Print the SHAP values for each instance in the dataset
for i in range(len(shap_values)):
    print("Instance:", i)
    for j in range(len(xx_train.columns)):
        print(xx_train.columns[j], ":", shap_values[j][i])
    print("\n")

# Calculate the expected value
expected_value = explainer.expected_value

# Print the expected value
print("Expected Value:", expected_value)